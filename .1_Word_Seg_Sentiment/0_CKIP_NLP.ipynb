{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eP1cM4OZFCuN"
   },
   "source": [
    "## **CKIPS WS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 對每年的新聞斷詞\n",
    "- news_all.xlsx\n",
    "- 使用colabtory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MltfthGo_UBS"
   },
   "source": [
    "### **Install Package**\n",
    "*   pip install\n",
    "*   import\n",
    "* connect to google drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72V3PHUoTzf_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ckiptagger[gdown,tf] in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: gdown in c:\\users\\user\\anaconda3\\lib\\site-packages (from ckiptagger[gdown,tf]) (5.2.0)\n",
      "Requirement already satisfied: tensorflow>=1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ckiptagger[gdown,tf]) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.16.2)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf])\n",
      "  Using cached keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.26.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown->ckiptagger[gdown,tf]) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown->ckiptagger[gdown,tf]) (3.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown->ckiptagger[gdown,tf]) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown->ckiptagger[gdown,tf]) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->gdown->ckiptagger[gdown,tf]) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.1.0)\n",
      "Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "Successfully installed keras-3.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U ckiptagger[tf,gdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1582087462410,
     "user": {
      "displayName": "林可佳",
      "photoUrl": "",
      "userId": "10564986521778912108"
     },
     "user_tz": -480
    },
    "id": "8cTvcW2qF97E",
    "outputId": "28d11890-159a-4415-a427-771f4ef3167e"
   },
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNw08X30a1x-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20584,
     "status": "ok",
     "timestamp": 1582087486666,
     "user": {
      "displayName": "林可佳",
      "photoUrl": "",
      "userId": "10564986521778912108"
     },
     "user_tz": -480
    },
    "id": "OxVUFSucG6Qu",
    "outputId": "1efa50dc-c659-43a3-a024-f0436562c381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hj8qvGd4_yV5"
   },
   "source": [
    "### **Set Path to WS, POS, NER**\n",
    "*   ws\n",
    "*   pos\n",
    "*   ner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyeQzeIyUEuq"
   },
   "outputs": [],
   "source": [
    "ws = WS(r\"C:\\Users\\USER\\Desktop\\Text Mining\\1_Word segmentation _ Sentiment\")\n",
    "pos = POS(r\"C:\\Users\\USER\\Desktop\\Text Mining\\1_Word segmentation _ Sentiment\")\n",
    "ner = NER(r\"C:\\Users\\USER\\Desktop\\Text Mining\\1_Word segmentation _ Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1582006858495,
     "user": {
      "displayName": "林可佳",
      "photoUrl": "",
      "userId": "10564986521778912108"
     },
     "user_tz": -480
    },
    "id": "yNNjag5mbGjX",
    "outputId": "f5c96a4f-6053-4f34-da01-8d709506c9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, {'公有': 2.0}), (3, {'土地公': 1.0, '土地婆': 1.0}), (5, {'緯來體育台': 1.0})]\n"
     ]
    }
   ],
   "source": [
    "word_to_weight = {\n",
    "    \"土地公\": 1,\n",
    "    \"土地婆\": 1,\n",
    "    \"公有\": 2,\n",
    "    \"\": 1,\n",
    "    \"來亂的\": \"啦\",\n",
    "    \"緯來體育台\": 1,\n",
    "}\n",
    "dictionary = construct_dictionary(word_to_weight)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZ2Y-Dvu9OLk"
   },
   "outputs": [],
   "source": [
    "sentence_list = [\"傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。\",\n",
    "                  \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。\",\n",
    "                  \"土地公有政策?？還是土地婆有政策。.\",\n",
    "                  \"… 你確定嗎… 不要再騙了……\",\n",
    "                  \"最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.\",\n",
    "                  \"科長說:1,坪數對人數為1:3。2,可以再增加。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiTAP-ad9LbK"
   },
   "outputs": [],
   "source": [
    "word_s = ws(sentence_list,\n",
    "            sentence_segmentation=True,\n",
    "            segment_delimiter_set={'?', '？', '!', '！', '。', ',',   \n",
    "                                   '，', ';', ':', '、'})\n",
    "# word_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mS__dh4h9npX"
   },
   "outputs": [],
   "source": [
    "word_p = pos(word_s)\n",
    "# word_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypBQK420-51F"
   },
   "outputs": [],
   "source": [
    "word_n = ner(word_s, word_p)\n",
    "# word_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDeFTzp7AFtS"
   },
   "source": [
    "### **Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUtT8FiFbKz0"
   },
   "outputs": [],
   "source": [
    "s_list = [\n",
    "    \"台灣證交所統計，本周（5月30日起至6月5日止）共計有3種有價證券將辦理公開申購抽籤作業，包括有6月1日：中天生物科技初次上櫃、上櫃增資股票尖點科技。6月5日則有上櫃增資股票磐亞。\",\n",
    "   \"即將於9日正式掛牌上櫃的中天生物科技股份有限公司（4128），昨（1）日進行電腦抽籤作業，8日並在晶華飯店舉行上櫃前法人說明會。成立於89年的中天生技科技，一直深耕於中草藥新藥與保健品兩大領域，主要產品除臨床中新藥外，共分為三大類，分別為共生發酵精華系列、李時珍生化本草系列及ODM系列；以該公司去年初所開發的「李時珍本草屋」系列生化本草精華保健品為例，為取自中藥的溫和藥材，以科學配方製成的養生保健營養品；自推出後，獲得中醫、藥界高度評價，銷售業績呈現大幅成長下，成為國內主要的中草藥保健品領導品牌之一。（楊明俊）\",\n",
    "]\n",
    "\n",
    "ws_list = ws(\n",
    "    s_list,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',',   \n",
    "                                   '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_list = pos(ws_list)\n",
    "\n",
    "es_list = ner(ws_list, ps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1582087579539,
     "user": {
      "displayName": "林可佳",
      "photoUrl": "",
      "userId": "10564986521778912108"
     },
     "user_tz": -480
    },
    "id": "bDBkPwc9w-wa",
    "outputId": "ac5c127f-5eaa-493f-8915-0d66499e4de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'台灣證交所統計，本周（5月30日起至6月5日止）共計有3種有價證券將辦理公開申購抽籤作業，包括有6月1日：中天生物科技初次上櫃、上櫃增資股票尖點科技。6月5日則有上櫃增資股票磐亞。'\n",
      "台灣(Nc)　證交所(Nc)　統計(Na)　，(COMMACATEGORY)　本(Nes)　周(Nd)　（(PARENTHESISCATEGORY)　5月(Nd)　30日(Nd)　起(Ng)　至(Caa)　6月(Nd)　5日(Nd)　止(Ng)　）(PARENTHESISCATEGORY)　共計(VJ)　有(V_2)　3(Neu)　種(Nf)　有價證券(Na)　將(D)　辦理(VC)　公開(VHC)　申購(VC)　抽籤(Nv)　作業(Na)　，(COMMACATEGORY)　包括(VK)　有(V_2)　6月(Nd)　1日(Nd)　：(COLONCATEGORY)　中天(Nb)　生物(Na)　科技(Na)　初次(D)　上櫃(VH)　、(PAUSECATEGORY)　上櫃(VH)　增資(VC)　股票(Na)　尖點(Na)　科技(Na)　。(PERIODCATEGORY)　6月(Nd)　5日(Nd)　則(D)　有(V_2)　上櫃(VH)　增資(VC)　股票(Na)　磐亞(Nb)　。(PERIODCATEGORY)　\n",
      "(0, 5, 'ORG', '台灣證交所')\n",
      "(8, 23, 'DATE', '本周（5月30日起至6月5日止')\n",
      "(48, 52, 'DATE', '6月1日')\n",
      "(53, 59, 'ORG', '中天生物科技')\n",
      "(75, 79, 'DATE', '6月5日')\n",
      "(87, 89, 'ORG', '磐亞')\n",
      "\n",
      "'即將於9日正式掛牌上櫃的中天生物科技股份有限公司（4128），昨（1）日進行電腦抽籤作業，8日並在晶華飯店舉行上櫃前法人說明會。成立於89年的中天生技科技，一直深耕於中草藥新藥與保健品兩大領域，主要產品除臨床中新藥外，共分為三大類，分別為共生發酵精華系列、李時珍生化本草系列及ODM系列；以該公司去年初所開發的「李時珍本草屋」系列生化本草精華保健品為例，為取自中藥的溫和藥材，以科學配方製成的養生保健營養品；自推出後，獲得中醫、藥界高度評價，銷售業績呈現大幅成長下，成為國內主要的中草藥保健品領導品牌之一。（楊明俊）'\n",
      "即將(D)　於(P)　9日(Nd)　正式(VH)　掛牌(VA)　上櫃(VH)　的(DE)　中天(Nb)　生物(Na)　科技(Na)　股份(Na)　有限公司(Nc)　（4128）(Neu)　，(COMMACATEGORY)　昨(Nd)　（1）(Nd)　日(Nd)　進行(VC)　電腦(Na)　抽籤(Nv)　作業(Na)　，(COMMACATEGORY)　8日(Nd)　並(Cbb)　在(P)　晶華(Nb)　飯店(Nc)　舉行(VC)　上櫃(VH)　前(Ng)　法人(Na)　說明會(Na)　。(PERIODCATEGORY)　成立(VC)　於(P)　89年(Nd)　的(DE)　中天(Nb)　生技(Na)　科技(Na)　，(COMMACATEGORY)　一直(D)　深耕(VA)　於(P)　中草藥(Na)　新(VH)　藥(Na)　與(Caa)　保健品(Na)　兩(Neu)　大(VH)　領域(Na)　，(COMMACATEGORY)　主要(A)　產品(Na)　除(P)　臨床(A)　中(Ng)　新(VH)　藥(Na)　外(Ng)　，(COMMACATEGORY)　共(Da)　分為(VG)　三(Neu)　大(VH)　類(Nf)　，(COMMACATEGORY)　分別(D)　為(VG)　共生(VH)　發酵(VH)　精華(Na)　系列(Na)　、(PAUSECATEGORY)　李時珍(Nb)　生化(Na)　本(Nes)　草(Na)　系列(Na)　及(Caa)　ODM(FW)　系列(Na)　；(SEMICOLONCATEGORY)　以(P)　該(Nes)　公司(Nc)　去年(Nd)　初(Ng)　所(D)　開發(VC)　的(DE)　「(PARENTHESISCATEGORY)　李時珍(Nb)　本(Nes)　草屋(Na)　」(PARENTHESISCATEGORY)　系列(Na)　生化(Na)　本草(Na)　精華(Na)　保健品(Na)　為(VG)　例(Na)　，(COMMACATEGORY)　為(P)　取自(VI)　中藥(Na)　的(DE)　溫和(VH)　藥材(Na)　，(COMMACATEGORY)　以(P)　科學(Na)　配方(Na)　製成(VG)　的(DE)　養生(VA)　保健(Na)　營養品(Na)　；(SEMICOLONCATEGORY)　自(P)　推出(VC)　後(Ng)　，(COMMACATEGORY)　獲得(VJ)　中醫(Na)　、(PAUSECATEGORY)　藥界(Nc)　高度(A)　評價(Na)　，(COMMACATEGORY)　銷售(VC)　業績(Na)　呈現(VJ)　大幅(D)　成長(VH)　下(Ng)　，(COMMACATEGORY)　成為(VG)　國內(Nc)　主要(A)　的(DE)　中草藥(Na)　保健品(Na)　領導(VC)　品牌(Na)　之(DE)　一(Neu)　。(PERIODCATEGORY)　（(PARENTHESISCATEGORY)　楊明俊(Nb)　）(PARENTHESISCATEGORY)　\n",
      "(3, 5, 'DATE', '9日')\n",
      "(12, 24, 'ORG', '中天生物科技股份有限公司')\n",
      "(25, 29, 'CARDINAL', '4128')\n",
      "(33, 34, 'CARDINAL', '1')\n",
      "(45, 47, 'DATE', '8日')\n",
      "(49, 53, 'FAC', '晶華飯店')\n",
      "(67, 70, 'DATE', '89年')\n",
      "(71, 77, 'ORG', '中天生技科技')\n",
      "(112, 113, 'CARDINAL', '三')\n",
      "(128, 131, 'PERSON', '李時珍')\n",
      "(138, 141, 'ORG', 'ODM')\n",
      "(148, 151, 'DATE', '去年初')\n",
      "(156, 159, 'PERSON', '李時珍')\n",
      "(180, 182, 'NORP', '中藥')\n",
      "(254, 257, 'PERSON', '楊明俊')\n"
     ]
    }
   ],
   "source": [
    "def print_word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "    print()\n",
    "    return\n",
    "    \n",
    "for i, sentence in enumerate(s_list):\n",
    "    print()\n",
    "    print(f\"'{sentence}'\")\n",
    "    print_word_pos_sentence(ws_list[i],  ps_list[i])\n",
    "    for entity in sorted(es_list[i]):\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGmJtx-GbN6g"
   },
   "outputs": [],
   "source": [
    "def word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    list_ = []\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        # print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "        list_.append(f\"{word}({pos})\")\n",
    "    return list_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBT3MB50EogG"
   },
   "source": [
    "## **News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74AOVYuBE9Ur"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from google.colab import files\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRjj51CfGvz_"
   },
   "outputs": [],
   "source": [
    "#去除金融股&中籤率0\n",
    "news = pd.read_excel('/content/drive/My Drive/Research_Data/news_all.xlsx')\n",
    "news = news[~news['name'].isin(['元大期', '群益期', '三商壽', '台名', '福邦證', '王道銀行', '致和證', '上海商銀', '富喬', '台嘉碩', '崇越電', '一零四'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d70V2mrPGxrZ"
   },
   "outputs": [],
   "source": [
    "news = news.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIjedYqwqqnk"
   },
   "outputs": [],
   "source": [
    "n_06 = news.loc['2006']\n",
    "n_07, n_08, n_09 = news.loc['2007'], news.loc['2008'], news.loc['2009']\n",
    "n_10, n_11, n_12 = news.loc['2010'], news.loc['2011'], news.loc['2012']\n",
    "n_13, n_14, n_15 = news.loc['2013'], news.loc['2014'], news.loc['2015']\n",
    "n_16, n_17, n_18 = news.loc['2016'], news.loc['2017'], news.loc['2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1582087602814,
     "user": {
      "displayName": "林可佳",
      "photoUrl": "",
      "userId": "10564986521778912108"
     },
     "user_tz": -480
    },
    "id": "Mf_DIw_4tfTJ",
    "outputId": "0887a823-62f4-480c-ffdc-c22d48c8936a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(430, 3) (490, 3) (232, 3) (286, 3)\n",
      "(403, 3) (404, 3) (520, 3)\n",
      "(516, 3) (590, 3) (629, 3)\n",
      "(458, 3) (293, 3) (418, 3)\n"
     ]
    }
   ],
   "source": [
    "print(n_06.shape, n_07.shape, n_08.shape, n_09.shape)\n",
    "print(n_10.shape, n_11.shape, n_12.shape)\n",
    "print(n_13.shape, n_14.shape, n_15.shape)\n",
    "print(n_16.shape, n_17.shape, n_18.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHRQNx6pxSF7"
   },
   "source": [
    "#### 2006~2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cx-Z0tBox1Jr"
   },
   "source": [
    "* 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnGzMnBvR-YJ"
   },
   "outputs": [],
   "source": [
    "s_06 = n_06['content'].to_list()\n",
    "\n",
    "ws_06 = ws(\n",
    "    s_06,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_06 = pos(ws_06)\n",
    "\n",
    "es_06 = ner(ws_06, ps_06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVpbFGNEZ2mr"
   },
   "outputs": [],
   "source": [
    "wp_06 = []\n",
    "for i, sentence in enumerate(s_06):\n",
    "    wp_list = word_pos_sentence(ws_06[i],  ps_06[i])\n",
    "    wp_06.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snbM82ykm72m"
   },
   "outputs": [],
   "source": [
    "n_06['word_seg'] = ws_06\n",
    "n_06['word_pos'] = wp_06\n",
    "n_06['word_es'] = es_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJv-Zh0_g3tz"
   },
   "outputs": [],
   "source": [
    "n_06.to_excel('n_06.xlsx') \n",
    "files.download('n_06.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCzeC_qJqoi-"
   },
   "source": [
    "* 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fl6ApySuvdmN"
   },
   "outputs": [],
   "source": [
    "s_07 = n_07['content'].to_list()\n",
    "\n",
    "ws_07 = ws(\n",
    "    s_07,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'})\n",
    "\n",
    "ps_07 = pos(ws_07)\n",
    "\n",
    "es_07 = ner(ws_07, ps_07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGUGa1FPvg9J"
   },
   "outputs": [],
   "source": [
    "wp_07 = []\n",
    "for i, sentence in enumerate(s_07):\n",
    "    wp_list = word_pos_sentence(ws_07[i],  ps_07[i])\n",
    "    wp_07.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZgqskx7viBP"
   },
   "outputs": [],
   "source": [
    "n_07['word_seg'] = ws_07\n",
    "n_07['word_pos'] = wp_07\n",
    "n_07['word_es'] = es_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1GIfzIqpp0R"
   },
   "outputs": [],
   "source": [
    "n_07.to_excel('n_07.xlsx') \n",
    "files.download('n_07.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvBp8brSwPgE"
   },
   "source": [
    "* 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AT6Jvk1rwOvI"
   },
   "outputs": [],
   "source": [
    "s_08 = n_08['content'].to_list()\n",
    "\n",
    "ws_08 = ws(\n",
    "    s_08,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_08 = pos(ws_08)\n",
    "\n",
    "es_08 = ner(ws_08, ps_08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23bjW-TewUDM"
   },
   "outputs": [],
   "source": [
    "wp_08 = []\n",
    "for i, sentence in enumerate(s_08):\n",
    "    wp_list = word_pos_sentence(ws_08[i],  ps_08[i])\n",
    "    wp_08.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs4fQ7F7wUDP"
   },
   "outputs": [],
   "source": [
    "n_08['word_seg'] = ws_08\n",
    "n_08['word_pos'] = wp_08\n",
    "n_08['word_es'] = es_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_1Ax9OovWIy"
   },
   "outputs": [],
   "source": [
    "n_08.to_excel('n_08.xlsx') \n",
    "files.download('n_08.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kg0-yzgwwVTC"
   },
   "source": [
    "* 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgXlyF8ZwVTD"
   },
   "outputs": [],
   "source": [
    "s_09 = n_09['content'].to_list()\n",
    "\n",
    "ws_09 = ws(\n",
    "    s_09,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_09 = pos(ws_09)\n",
    "\n",
    "es_09 = ner(ws_09, ps_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCzmd-wzwVTF"
   },
   "outputs": [],
   "source": [
    "wp_09 = []\n",
    "for i, sentence in enumerate(s_09):\n",
    "    wp_list = word_pos_sentence(ws_09[i],  ps_09[i])\n",
    "    wp_09.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okYzrYT-wVTH"
   },
   "outputs": [],
   "source": [
    "n_09['word_seg'] = ws_09\n",
    "n_09['word_pos'] = wp_09\n",
    "n_09['word_es'] = es_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw99SJSpvb4u"
   },
   "outputs": [],
   "source": [
    "n_09.to_excel('n_09.xlsx') \n",
    "files.download('n_09.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgoXZ0e6wXu0"
   },
   "source": [
    "* 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbP67jY3wXu1"
   },
   "outputs": [],
   "source": [
    "s_10 = n_10['content'].to_list()\n",
    "\n",
    "ws_10 = ws(\n",
    "    s_10,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_10 = pos(ws_10)\n",
    "\n",
    "es_10 = ner(ws_10, ps_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xjwfjTlwXu3"
   },
   "outputs": [],
   "source": [
    "wp_10 = []\n",
    "for i, sentence in enumerate(s_10):\n",
    "    wp_list = word_pos_sentence(ws_10[i],  ps_10[i])\n",
    "    wp_10.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgMoFEolwXu6"
   },
   "outputs": [],
   "source": [
    "n_10['word_seg'] = ws_10\n",
    "n_10['word_pos'] = wp_10\n",
    "n_10['word_es'] = es_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rN5fnZj3vdH1"
   },
   "outputs": [],
   "source": [
    "n_10.to_excel('n_10.xlsx') \n",
    "files.download('n_10.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAY8M5PswZUp"
   },
   "source": [
    "#### 2011~2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urHvbpIvyDN8"
   },
   "source": [
    "* 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOA95WrxwZUq"
   },
   "outputs": [],
   "source": [
    "s_11 = n_11['content'].to_list()\n",
    "\n",
    "ws_11 = ws(\n",
    "    s_11,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_11 = pos(ws_11)\n",
    "\n",
    "es_11 = ner(ws_11, ps_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IExFL9KAwZUs"
   },
   "outputs": [],
   "source": [
    "wp_11 = []\n",
    "for i, sentence in enumerate(s_11):\n",
    "    wp_list = word_pos_sentence(ws_11[i],  ps_11[i])\n",
    "    wp_11.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6dOe0NUwZUu"
   },
   "outputs": [],
   "source": [
    "# n_11['word_seg'] = ws_11\n",
    "# n_11['word_pos'] = wp_11\n",
    "n_11['word_es'] = es_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EC3GKXwvdzC"
   },
   "outputs": [],
   "source": [
    "n_11.to_excel('n_11.xlsx') \n",
    "files.download('n_11.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1q1t0eiwcsj"
   },
   "source": [
    "* 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwxnnrISwcsk"
   },
   "outputs": [],
   "source": [
    "s_12 = n_12['content'].to_list()\n",
    "\n",
    "ws_12 = ws(\n",
    "    s_12,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_12 = pos(ws_12)\n",
    "\n",
    "es_12 = ner(ws_12, ps_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9XASu5Wwcsm"
   },
   "outputs": [],
   "source": [
    "wp_12 = []\n",
    "for i, sentence in enumerate(s_12):\n",
    "    wp_list = word_pos_sentence(ws_12[i],  ps_12[i])\n",
    "    wp_12.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QcK4SFAwcso"
   },
   "outputs": [],
   "source": [
    "# n_12['word_seg'] = ws_12\n",
    "# n_12['word_pos'] = wp_12\n",
    "n_12['word_es'] = es_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ld_z0QhyveH4"
   },
   "outputs": [],
   "source": [
    "n_12.to_excel('n_12.xlsx') \n",
    "files.download('n_12.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q523EMK6witC"
   },
   "source": [
    "* 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReLiRC8RwitJ"
   },
   "outputs": [],
   "source": [
    "s_13 = n_13['content'].to_list()\n",
    "\n",
    "ws_13 = ws(\n",
    "    s_13,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_13 = pos(ws_13)\n",
    "\n",
    "es_13 = ner(ws_13, ps_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rc-W778UwitN"
   },
   "outputs": [],
   "source": [
    "wp_13 = []\n",
    "for i, sentence in enumerate(s_13):\n",
    "    wp_list = word_pos_sentence(ws_13[i],  ps_13[i])\n",
    "    wp_13.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwh3DS_1witR"
   },
   "outputs": [],
   "source": [
    "n_13['word_seg'] = ws_13\n",
    "n_13['word_pos'] = wp_13\n",
    "n_13['word_es'] = es_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITY8LjhXveyG"
   },
   "outputs": [],
   "source": [
    "n_13.to_excel('n_13.xlsx') \n",
    "files.download('n_13.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iq6qHoJHwitT"
   },
   "source": [
    "* 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOzJFLjlwitT"
   },
   "outputs": [],
   "source": [
    "s_14 = n_14['content'].to_list()\n",
    "\n",
    "ws_14 = ws(\n",
    "    s_14,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_14 = pos(ws_14)\n",
    "\n",
    "es_14 = ner(ws_14, ps_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3FrhPbwwitW"
   },
   "outputs": [],
   "source": [
    "wp_14 = []\n",
    "for i, sentence in enumerate(s_14):\n",
    "    wp_list = word_pos_sentence(ws_14[i],  ps_14[i])\n",
    "    wp_14.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qk8WACiEwitY"
   },
   "outputs": [],
   "source": [
    "# n_14['word_seg'] = ws_14\n",
    "# n_14['word_pos'] = wp_14\n",
    "n_14['word_es'] = es_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mj60yL_6vfQf"
   },
   "outputs": [],
   "source": [
    "n_14.to_excel('n_14.xlsx') \n",
    "files.download('n_14.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ovoU0atwlVQ"
   },
   "source": [
    "* 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdnFpYGRwlVR"
   },
   "outputs": [],
   "source": [
    "s_15 = n_15['content'].to_list()\n",
    "\n",
    "ws_15 = ws(\n",
    "    s_15,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_15 = pos(ws_15)\n",
    "\n",
    "es_15 = ner(ws_15, ps_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWEEtxS1wlVU"
   },
   "outputs": [],
   "source": [
    "wp_15 = []\n",
    "for i, sentence in enumerate(s_15):\n",
    "    wp_list = word_pos_sentence(ws_15[i],  ps_15[i])\n",
    "    wp_15.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONKA8RPHwlVW"
   },
   "outputs": [],
   "source": [
    "n_15['word_seg'] = ws_15\n",
    "n_15['word_pos'] = wp_15\n",
    "n_15['word_es'] = es_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIZCqPPuvfng"
   },
   "outputs": [],
   "source": [
    "n_15.to_excel('n_15.xlsx') \n",
    "files.download('n_15.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRuZTLYqwlVY"
   },
   "source": [
    "\n",
    "#### 2016~2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXQ3kLz2yQk5"
   },
   "source": [
    "* 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeyqeM9owlVY"
   },
   "outputs": [],
   "source": [
    "s_16 = n_16['content'].to_list()\n",
    "\n",
    "ws_16 = ws(\n",
    "    s_16,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_16 = pos(ws_16)\n",
    "\n",
    "es_16 = ner(ws_16, ps_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqNIcfCjwlVa"
   },
   "outputs": [],
   "source": [
    "wp_16 = []\n",
    "for i, sentence in enumerate(s_16):\n",
    "    wp_list = word_pos_sentence(ws_16[i],  ps_16[i])\n",
    "    wp_16.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xei9nnfPwlVc"
   },
   "outputs": [],
   "source": [
    "n_16['word_seg'] = ws_16\n",
    "n_16['word_pos'] = wp_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhTY5QlwvgOQ"
   },
   "outputs": [],
   "source": [
    "n_16.to_excel('n_16.xlsx') \n",
    "files.download('n_16.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXFuIspMwoLz"
   },
   "source": [
    "* 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVUSolitwoL0"
   },
   "outputs": [],
   "source": [
    "s_17 = n_17['content'].to_list()\n",
    "\n",
    "ws_17 = ws(\n",
    "    s_17,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_17 = pos(ws_17)\n",
    "\n",
    "es_17 = ner(ws_17, ps_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPjzzk3ywoL2"
   },
   "outputs": [],
   "source": [
    "wp_17 = []\n",
    "for i, sentence in enumerate(s_17):\n",
    "    wp_list = word_pos_sentence(ws_17[i],  ps_17[i])\n",
    "    wp_17.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEDvxB0LwoL4"
   },
   "outputs": [],
   "source": [
    "n_17['word_seg'] = ws_17\n",
    "n_17['word_pos'] = wp_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_Ucr35gvgvB"
   },
   "outputs": [],
   "source": [
    "n_17.to_excel('n_17.xlsx') \n",
    "files.download('n_17.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3PACgvl6woL6"
   },
   "source": [
    "* 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuMXxhiqwoL7"
   },
   "outputs": [],
   "source": [
    "s_18 = n_18['content'].to_list()\n",
    "\n",
    "ws_18 = ws(\n",
    "    s_18,\n",
    "    sentence_segmentation=True, # To consider delimiters\n",
    "    segment_delimiter_set = {'?', '？', '!', '！', '。', ',', '，', ';', ':', '、', '：', '；', '】', '【'}, # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "\n",
    "ps_18 = pos(ws_18)\n",
    "\n",
    "es_18 = ner(ws_18, ps_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyvWsbqNwoL9"
   },
   "outputs": [],
   "source": [
    "wp_18 = []\n",
    "for i, sentence in enumerate(s_18):\n",
    "    wp_list = word_pos_sentence(ws_18[i],  ps_18[i])\n",
    "    wp_18.append(wp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHzRbX71woL_"
   },
   "outputs": [],
   "source": [
    "n_18['word_seg'] = ws_18\n",
    "n_18['word_pos'] = wp_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeMKCqG4vhMK"
   },
   "outputs": [],
   "source": [
    "n_18.to_excel('n_18.xlsx') \n",
    "files.download('n_18.xlsx')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPIhAb1DTtScadM5ucGlLYG",
   "collapsed_sections": [],
   "name": "CKIP NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
